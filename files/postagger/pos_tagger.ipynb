{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter POS Tagging \n",
    "The goal of this tutorial is to introduce a a Part-of-Speech (POS) tagger developed for tweets which was released as part of the [TweetNLP](https://www.ark.cs.cmu.edu/TweetNLP/) toolkit. The code is written in Java and the python wrapper for the tokenization is from [this](https://github.com/myleott/ark-twokenize-py) github repository. This tutorial has code from the [TweetNLP](https://github.com/brendano/ark-tweet-nlp/) github repository as well as the python wrapper from [this](https://github.com/ianozsvald/ark-tweet-nlp-python) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "- POS tagging involves identifying part-of-speech of tokens in a given text. This can be viewed as a task of labeling the sentence w_1, w_2, ....., w_n with pos tags, one for each word: t_1, t_2, ...., t_n.\n",
    "- The 8 common parts of speech for english language are:\n",
    "  1. Noun\n",
    "  2. Verb\n",
    "  3. Pronoun\n",
    "  4. Preposition\n",
    "  5. Adverb\n",
    "  6. Conjuction\n",
    "  7. Participle\n",
    "  8. Article  \n",
    "- Twitter data is different from standard language data in that there are tokens such as #, @, emoticons, URLs, etc. So the tagset for twitter needs to incorporate the tags for these new tokens. The tags that are used to annotate tweets are as follows:\n",
    "\n",
    "<img src=\"pos_tags.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments\n",
    "- This tutorial covers how to accomplish the task of POS tagging for twitter data based on this paper: https://aclanthology.org/P11-2008.pdf\n",
    "- The nature of twitter data poses challenges in using standard POS taggers. The paper develops the above tagset for twitter to include tags for words that are not commonly encountered in language outside of twitter. \n",
    "- Around 1,800 tweets were manually annotated with corresponding pos tags.\n",
    "- Conditional Random Fields (CRFs) were used with features specific to twitter POS tagging. The features for the CRF are below (see paper for more details):\n",
    "  - Twitter orthography - these features are rules that detect @, #, and URls.\n",
    "  - Names - these features check for names from a dictionary of compiled tokens which are frequently capitalized.\n",
    "  - Traditional Tag Dictionary - these are features for all tags that occur in PTB.\n",
    "  - Distributional Similarity - these features are constructed from the successor and predecessor probabilities for the 10,000 most common terms.\n",
    "  - Phonetic normalization - words are normalized to ignore alternate spellings of words using the Metaphone algorithm; e.x.{thangs, thanks, thanksss, thanx, thinks, thnx} are mapped to 0NKS.\n",
    "- 1827 tweets that are annotated are divided into training set of 1000 tweets, dev set of 327 tweets, and test set of 500 tweets. The results of the tagger incorporating the above features are compared with the standard Stanford Tagger and using the above feature set for twitter data reduces error by about 25%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "To run the tutorial you will need to download the POS tagger from https://code.google.com/archive/p/ark-tweet-nlp/downloads\n",
    "\n",
    "This requires Java 6. https://www.oracle.com/java/technologies/java-platform.html\n",
    "\n",
    "Place this ipython notebook that has python wrappers inside the ark-tweet-nlp-0.3.2 folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step would be to download packages required for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "try:\n",
    "    from html.parser import HTMLParser\n",
    "except ImportError:\n",
    "    from HTMLParser import HTMLParser\n",
    "  \n",
    "\n",
    "try:\n",
    "    import html\n",
    "except ImportError:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Wrapper for POS Tagger\n",
    "- The functions below call the runTagger.sh to get the POS tag predictions for the tokenized tweets. \n",
    "- runTagger.sh script should be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TAGGER_CMD = \"java -XX:ParallelGCThreads=2 -Xmx500m -jar ark-tweet-nlp-0.3.2.jar\"\n",
    "\n",
    "def _split_results(rows):\n",
    "    \"\"\"Parse the tab-delimited returned lines, modified from: https://github.com/brendano/ark-tweet-nlp/blob/master/scripts/show.py\"\"\"\n",
    "    for line in rows:\n",
    "        line = line.strip()  # remove '\\n'\n",
    "        if len(line) > 0:\n",
    "            if line.count('\\t') == 2:\n",
    "                parts = line.split('\\t')\n",
    "                tokens = parts[0]\n",
    "                tags = parts[1]\n",
    "                confidence = float(parts[2])\n",
    "                yield tokens, tags, confidence\n",
    "                \n",
    "                \n",
    "def _call_runtagger(tweets, run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Call runTagger.sh using a named input file\"\"\"\n",
    "\n",
    "    # remove carriage returns as they are tweet separators for the stdin\n",
    "    # interface\n",
    "    tweets_cleaned = [tw.replace('\\n', ' ') for tw in tweets]\n",
    "    message = \"\\n\".join(tweets_cleaned)\n",
    "\n",
    "    # force UTF-8 encoding (from internal unicode type) to avoid .communicate encoding error as per:\n",
    "    # http://stackoverflow.com/questions/3040101/python-encoding-for-pipe-communicate\n",
    "    message = message.encode('utf-8')\n",
    "\n",
    "    # build a list of args\n",
    "    args = shlex.split(run_tagger_cmd)\n",
    "    args.append('--output-format')\n",
    "    args.append('conll')\n",
    "    po = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    # old call - made a direct call to runTagger.sh (not Windows friendly)\n",
    "    #po = subprocess.Popen([run_tagger_cmd, '--output-format', 'conll'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    result = po.communicate(message)\n",
    "    # expect a tuple of 2 items like:\n",
    "    # ('hello\\t!\\t0.9858\\nthere\\tR\\t0.4168\\n\\n',\n",
    "    # 'Listening on stdin for input.  (-h for help)\\nDetected text input format\\nTokenized and tagged 1 tweets (2 tokens) in 7.5 seconds: 0.1 tweets/sec, 0.3 tokens/sec\\n')\n",
    "\n",
    "    pos_result = result[0].decode('utf-8').strip('\\n\\n')  # get first line, remove final double carriage return\n",
    "    pos_result = pos_result.split('\\n\\n')  # split messages by double carriage returns\n",
    "    pos_results = [pr.split('\\n') for pr in pos_result]  # split parts of message by each carriage return\n",
    "    return pos_results\n",
    "\n",
    "\n",
    "def runtagger_parse(tweets, run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Call runTagger.sh on a list of tweets, parse the result, return lists of tuples of (term, type, confidence)\"\"\"\n",
    "    pos_raw_results = _call_runtagger(tweets, run_tagger_cmd)\n",
    "    pos_result = []\n",
    "    for pos_raw_result in pos_raw_results:\n",
    "        pos_result.append([x for x in _split_results(pos_raw_result)])\n",
    "    return pos_result\n",
    "\n",
    "\n",
    "def check_script_is_present(run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Simple test to make sure we can see the script\"\"\"\n",
    "    success = False\n",
    "    try:\n",
    "        args = shlex.split(run_tagger_cmd)\n",
    "        args.append(\"--help\")\n",
    "        po = subprocess.Popen(args, stdout=subprocess.PIPE)\n",
    "        # old call - made a direct call to runTagger.sh (not Windows friendly)\n",
    "        #po = subprocess.Popen([run_tagger_cmd, '--help'], stdout=subprocess.PIPE)\n",
    "        while not po.poll():\n",
    "            lines = [l for l in po.stdout]\n",
    "        # we expected the first line of --help to look like the following:\n",
    "        assert \"RunTagger [options]\" in lines[0].decode('utf-8')\n",
    "        success = True\n",
    "    except OSError as err:\n",
    "        print(\"Caught an OSError, have you specified the correct path to runTagger.sh? We are using \\\"%s\\\". Exception: %r\" % (run_tagger_cmd, repr(err)))\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read tokenized tweets\n",
    "We will now load tweets that have the tokenized for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I won't win a single game I bet on !! Got Mr. Cliff Lee , if he loses its on me U.S.A ! $ 5 0.0 .\\n\", '@thecamion I like monkeys , but I still hate COSTCO parking lots .. oO o.O #COSTCO 2:15 PM\\n', 'RT @eye_e : this poster-print costs $ 12 . 40 , which is 40% of the normal price ! http://tl.gd/6meogh\\n', 'LMBO ! This man filed an EMERGENCY Motion for Continuance on account of the Rangers game tonight !\\n', 'Texas Rangers are in the World Series ! Go Rangers !!!!!!!!! : > <3 ♥❤♡ http://fb.me/D2LsXBJx\\n']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"tweets_tokenized.txt\", \"r\")\n",
    "tweets_tokenized = file.readlines()\n",
    "print(tweets_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply POS tagger\n",
    "The output of the POS tagger is a tuple containing token, predicted output tag, and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'O', 0.9942), (\"won't\", 'V', 0.9982), ('win', 'V', 0.9993), ('a', 'D', 0.9987), ('single', 'A', 0.9758), ('game', 'N', 0.9988), ('I', 'O', 0.9922), ('bet', 'V', 0.9995), ('on', 'P', 0.9162), ('!!', ',', 0.9873), ('Got', 'V', 0.9965), ('Mr.', '^', 0.9863), ('Cliff', '^', 0.9993), ('Lee', '^', 0.9999), (',', ',', 0.9975), ('if', 'P', 0.9987), ('he', 'O', 0.9979), ('loses', 'V', 0.9996), ('its', 'L', 0.9865), ('on', 'P', 0.9871), ('me', 'O', 0.9823), ('U.S.A', '^', 0.7232), ('!', ',', 0.9442), ('$', 'G', 0.7774), ('5', '$', 0.9919), ('0.0', '$', 0.9866), ('.', ',', 0.993)]]\n",
      "press enter ...\n",
      "[[('@thecamion', '@', 0.9995), ('I', 'O', 0.9953), ('like', 'V', 0.9027), ('monkeys', 'N', 0.9408), (',', ',', 0.999), ('but', '&', 0.9974), ('I', 'O', 0.9988), ('still', 'R', 0.9858), ('hate', 'V', 0.9938), ('COSTCO', '^', 0.953), ('parking', 'N', 0.6863), ('lots', 'N', 0.9851), ('..', ',', 0.9939), ('oO', '!', 0.9601), ('o.O', 'E', 0.7939), ('#COSTCO', '^', 0.5895), ('2:15', '$', 0.9761), ('PM', 'N', 0.9566)]]\n",
      "press enter ...\n",
      "[[('RT', '~', 0.9979), ('@eye_e', '@', 0.9992), (':', '~', 0.9756), ('this', 'D', 0.9815), ('poster-print', 'N', 0.9922), ('costs', 'N', 0.6722), ('$', 'G', 0.6476), ('12', '$', 0.9983), ('.', ',', 0.9927), ('40', '$', 0.9862), (',', ',', 0.9968), ('which', 'O', 0.6752), ('is', 'V', 0.9986), ('40%', '$', 0.9843), ('of', 'P', 0.9985), ('the', 'D', 0.9996), ('normal', 'A', 0.9925), ('price', 'N', 0.9986), ('!', ',', 0.998), ('http://tl.gd/6meogh', 'U', 0.9952)]]\n",
      "press enter ...\n",
      "[[('LMBO', '!', 0.9982), ('!', ',', 0.9982), ('This', 'D', 0.9916), ('man', 'N', 0.9892), ('filed', 'V', 0.9713), ('an', 'D', 0.9861), ('EMERGENCY', 'N', 0.9715), ('Motion', 'N', 0.9859), ('for', 'P', 0.9995), ('Continuance', 'N', 0.9229), ('on', 'P', 0.9955), ('account', 'N', 0.9991), ('of', 'P', 0.9995), ('the', 'D', 0.999), ('Rangers', '^', 0.9495), ('game', 'N', 0.9765), ('tonight', 'N', 0.623), ('!', ',', 0.999)]]\n",
      "press enter ...\n",
      "[[('Texas', '^', 0.9984), ('Rangers', '^', 0.9582), ('are', 'V', 0.9913), ('in', 'P', 0.9947), ('the', 'D', 0.999), ('World', '^', 0.8136), ('Series', '^', 0.8281), ('!', ',', 0.9896), ('Go', 'V', 0.987), ('Rangers', '^', 0.9369), ('!!!!!!!!!', ',', 0.9808), (':', ',', 0.7083), ('>', 'E', 0.6507), ('<3', 'E', 0.998), ('♥❤♡', 'E', 0.5116), ('http://fb.me/D2LsXBJx', 'U', 0.988)]]\n",
      "press enter ...\n"
     ]
    }
   ],
   "source": [
    "# print(RUN_TAGGER_CMD)\n",
    "# success = check_script_is_present()\n",
    "# if success:\n",
    "\n",
    "inp_file = open('tweets_tokenized.txt')\n",
    "for t in inp_file.readlines():\n",
    "    print(runtagger_parse([t]))    \n",
    "    input(\"press enter ...\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
